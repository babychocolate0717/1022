import os
import random
import numpy as np
import pandas as pd
import joblib
from pathlib import Path
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from urllib.parse import urlparse
from datetime import datetime, timezone

# ========= éš¨æ©Ÿç¨®å­ï¼ˆå¯é‡ç¾ï¼‰ =========
np.random.seed(42)
random.seed(42)
tf.random.set_seed(42)

# ========= å…ˆè¼‰å…¥ .envï¼Œå†å–ç’°å¢ƒè®Šæ•¸ =========
load_dotenv() 

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
DB_URL = os.getenv("DB_URL") or os.getenv("DATABASE_URL")
STEP_MIN = int(os.getenv("STEP_MINUTES", "1"))
# ä¿®æ­£ 1ï¼šå®šç¾© WINDOW å¯¦é©—ç¯„åœ
WINDOW_LIST = [60, 90, 120, 180, 300] 
WINDOW = int(os.getenv("WINDOW", "60"))
EPOCHS = int(os.getenv("EPOCHS", "500"))
BATCH = int(os.getenv("BATCH", "128"))

# å¯é¸è¨“ç·´æœŸé–“åƒæ•¸
TRAIN_DAYS = os.getenv("TRAIN_DAYS")
TRAIN_START_UTC = os.getenv("TRAIN_START_UTC")
TRAIN_END_UTC = os.getenv("TRAIN_END_UTC")

# ----------------------------------------------------------------------
# *** é—œéµä¿®æ­£ï¼šè§£æ±º DB_URL ä¸»æ©Ÿåå•é¡Œ (ä½¿ Windows ä¸»æ©Ÿå¯ä»¥é€£ç·š) ***
# ----------------------------------------------------------------------
if DB_URL and 'db:5432' in DB_URL:
    DB_URL = DB_URL.replace("db:5432", "localhost:5433")
    print("WARNING: DB_URL host swapped to localhost:5433 for local execution.")
# ----------------------------------------------------------------------

if not DB_URL:
    raise RuntimeError("DB_URL æœªè¨­å®šï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆã€‚")
u = urlparse(DB_URL)
print("Using DB_URL:", f"{u.scheme}://{u.username}:******@{u.hostname}:{u.port}{u.path}")
print(f"STEP_MIN={STEP_MIN}, EPOCHS={EPOCHS}, BATCH={BATCH}") # ç§»é™¤ WINDOW æ‰“å°ï¼Œå› ç‚º WINDOWs æ˜¯è¿­ä»£çš„

# ========= æ¬„ä½åç¨± =========
TIME_COL = "timestamp"
PWR_COL = "system_power_watt"
N_FEATURES = 4 # 4 å€‹ç‰¹å¾µ (power_w, hour, dayofweek, lag_24h)

# ========= SQLï¼ˆè®€å–è³‡æ–™ï¼‰ =========
where_extra, params = [], {}
if TRAIN_DAYS and TRAIN_DAYS.isdigit():
    where_extra.append("(timestamp_utc)::timestamptz >= now() - interval :train_days")
    params["train_days"] = f"{int(TRAIN_DAYS)} days"
if TRAIN_START_UTC:
    where_extra.append("(timestamp_utc)::timestamptz >= :start_utc")
    params["start_utc"] = TRAIN_START_UTC
if TRAIN_END_UTC:
    where_extra.append("(timestamp_utc)::timestamptz <= :end_utc")
    params["end_utc"] = TRAIN_END_UTC

where_clause = (" AND " + " AND ".join(where_extra) + "\n") if where_extra else ""

SQL = f"""
SELECT
  (timestamp_utc)::timestamptz AS {TIME_COL},
  {PWR_COL}
FROM energy_cleaned
WHERE timestamp_utc IS NOT NULL
  AND {PWR_COL} IS NOT NULL
{where_clause}ORDER BY (timestamp_utc)::timestamptz
""".strip()
print("DEBUG SQL:\n", SQL)

# ========= è®€å–è³‡æ–™ä¸¦é€²è¡ŒåŸºç¤Žæ¸…æ´— (æ•¸æ“šåªè®€å–ä¸€æ¬¡) =========
engine = create_engine(DB_URL, pool_pre_ping=True)
df_raw = pd.read_sql(text(SQL), engine, params=params, parse_dates=[TIME_COL])
if df_raw.empty:
    raise RuntimeError("energy_cleaned æŸ¥ç„¡è³‡æ–™ï¼ˆç¬¦åˆæ¢ä»¶ç‚º 0ï¼‰ã€‚")

print(f"Loaded rows: {len(df_raw)} | time span: {df_raw[TIME_COL].min()} â†’ {df_raw[TIME_COL].max()}")

# ========= æ•¸æ“šæº–å‚™å‡½å¼ (åœ¨å¾ªç’°å…§èª¿ç”¨) =========

def prepare_data(df_input, window_size):
    """åŸ·è¡Œç‰¹å¾µå·¥ç¨‹ã€å°é½Šã€åˆ‡åˆ†å’Œæ­£è¦åŒ–"""
    df = df_input.set_index(TIME_COL).sort_index().copy()
    df = df.resample(f"{STEP_MIN}min").mean()
    df[PWR_COL] = df[PWR_COL].ffill().bfill()
    
    # å°–å³°å¹³æ»‘
    q1, q3 = df[PWR_COL].quantile([0.25, 0.75])
    iqr = q3 - q1
    lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr
    df[PWR_COL] = df[PWR_COL].clip(lower, upper)
    df[PWR_COL] = df[PWR_COL].rolling(window=3, center=True, min_periods=1).median()

    # >>> ç‰¹å¾µå·¥ç¨‹ <<<
    df.index = pd.to_datetime(df.index)
    df['hour'] = df.index.hour.astype(float)
    df['dayofweek'] = df.index.dayofweek.astype(float)
    
    # Day-Ahead Lag ç‰¹å¾µ (1440 åˆ†é˜)
    LAG_MINUTES = 24 * 60 
    df['power_lag_24h'] = df[PWR_COL].shift(LAG_MINUTES)
    df = df.dropna()

    features = df[[PWR_COL, 'hour', 'dayofweek', 'power_lag_24h']].astype(float).values

    if len(features) < window_size + 2:
        # è‡ªå‹•èª¿æ•´ WINDOW é‚è¼¯ (ç”±æ–¼æ˜¯å¯¦é©—ï¼Œç›´æŽ¥è·³éŽæˆ–å ±éŒ¯)
        raise RuntimeError(f"æ•¸æ“šé‡ä¸è¶³ä»¥å»ºç«‹ WINDOW={window_size} çš„åºåˆ—ã€‚")

    # Scaling (Scaler ä½¿ç”¨æ‰€æœ‰ N_FEATURES=4)
    scaler = MinMaxScaler()
    split_idx = int(len(features) * 0.8)
    features_tr, features_val = features[:split_idx], features[split_idx:]
    
    scaler.fit(features_tr)
    features_tr_scaled  = scaler.transform(features_tr)
    features_val_scaled = scaler.transform(features_val)
    
    # å»ºç«‹åºåˆ—
    X_tr, y_tr   = make_sequences(features_tr_scaled, window_size, N_FEATURES)
    X_val, y_val = make_sequences(features_val_scaled, window_size, N_FEATURES)

    if len(X_tr) == 0 or len(X_val) == 0:
        raise RuntimeError("åˆ‡åˆ†å¾Œç„¡æ³•å½¢æˆåºåˆ—ï¼Œè«‹èª¿æ•´ WINDOWã€‚")
    
    return X_tr, y_tr, X_val, y_val, scaler, len(features), len(X_tr), len(X_val)

def make_sequences(arr, window, n_features):
    X, y = [], []
    for i in range(len(arr) - window - 1):
        X.append(arr[i:i+window])
        y.append(arr[i+window, 0]) 
    return np.array(X).reshape(-1, window, n_features), np.array(y).reshape(-1, 1)


def train_and_evaluate(window_size, df_full, out_dir):
    """åŸ·è¡Œå–®æ¬¡è¨“ç·´ã€è©•ä¼°ä¸¦å„²å­˜çµæžœ"""
    print(f"--- æ­£åœ¨æ¸¬è©¦ WINDOW = {window_size} ---")
    try:
        # 1. æº–å‚™æ•¸æ“š
        X_tr, y_tr, X_val, y_val, scaler, n_raw, n_tr, n_val = prepare_data(df_full, window_size)
        
        # 2. å»ºæ¨¡ (Stacked LSTM)
        model = Sequential([
            LSTM(128, input_shape=(window_size, N_FEATURES), return_sequences=True), 
            Dropout(0.2),
            LSTM(64, return_sequences=False),
            Dropout(0.2),
            Dense(1)
        ])
        model.compile(optimizer="adam", loss="mse")

        # 3. è¨“ç·´ï¼ˆå¢žåŠ  Patienceï¼‰
        es  = EarlyStopping(patience=15, restore_best_weights=True)
        rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-5, verbose=0)

        history = model.fit(
            X_tr, y_tr, validation_data=(X_val, y_val),
            epochs=EPOCHS, batch_size=BATCH, callbacks=[es, rlr], verbose=0
        )
        
        # 4. è©•ä¼°
        y_pred = model.predict(X_val, verbose=0)
        y_temp = np.zeros((len(y_val), N_FEATURES)); y_pred_temp = np.zeros((len(y_pred), N_FEATURES))
        y_temp[:, 0] = y_val.flatten(); y_pred_temp[:, 0] = y_pred.flatten()
        y_val_w = scaler.inverse_transform(y_temp)[:, 0]
        y_pred_w = scaler.inverse_transform(y_pred_temp)[:, 0]

        rmse = float(np.sqrt(mean_squared_error(y_val_w, y_pred_w)))
        mape = float(mean_absolute_percentage_error(y_val_w, y_pred_w) * 100)
        best_val_loss = float(np.min(history.history["val_loss"]))
        
        # 5. å„²å­˜æœ€ä½³æ¨¡åž‹ (åœ¨å¯¦é©—ä¸­ï¼Œåªåœ¨æœ€ä½³é‹è¡Œæ™‚å„²å­˜)
        # é€™è£¡æˆ‘å€‘åªè¼¸å‡ºæ—¥èªŒï¼Œä¸è¦†è“‹ä¸»æ¨¡åž‹

        return rmse, mape, best_val_loss, n_tr, n_val, model, scaler

    except RuntimeError as e:
        print(f"--- WINDOW {window_size} å¤±æ•—: {e} ---")
        return None, None, None, None, None, None, None


# ----------------------------------------------------
# *** å¯¦é©—æŽ§åˆ¶ä¸­å¿ƒ (è…³æœ¬é‹è¡Œå…¥å£) ***
# ----------------------------------------------------
if __name__ == "__main__":
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    out_dir = Path("/models")
    if not out_dir.exists():
        out_dir = Path(__file__).resolve().parents[1] / "models"
    out_dir.mkdir(parents=True, exist_ok=True)
    
    # åŸ·è¡Œæ‰€æœ‰è¨“ç·´ (æ•¸æ“šåªè®€å–ä¸€æ¬¡)
    df_input = df_raw.copy()
    
    BEST_RMSE = float('inf')
    BEST_WINDOW = 0
    FINAL_RESULTS = []

    print("\n\n======== å•Ÿå‹• WINDOW æ•æ„Ÿåº¦å¯¦é©— ========")
    for W in WINDOW_LIST:
        rmse, mape, val_loss, n_tr, n_val, model_run, scaler_run = train_and_evaluate(W, df_input, out_dir)
        
        if rmse is not None:
            FINAL_RESULTS.append({
                'window': W, 
                'rmse_w': f"{rmse:.3f}", 
                'mape_percent': f"{mape:.2f}%", 
                'n_train': n_tr,
                'n_val': n_val
            })
            
            if rmse < BEST_RMSE:
                BEST_RMSE = rmse
                BEST_WINDOW = W
                
                # å„²å­˜æœ€ä½³é‹è¡Œçµæžœçš„æ¨¡åž‹å’Œ Scaler (è¦†è“‹ä¸»éƒ¨ç½²æª”æ¡ˆ)
                model_run.save(out_dir / "lstm_carbon_model.keras") 
                joblib.dump(scaler_run, out_dir / "scaler_power.pkl")
                print(f"ðŸ† ç™¼ç¾æœ€ä½³æ€§èƒ½ï¼æ¨¡åž‹å·²å„²å­˜ (RMSE: {BEST_RMSE:.3f}W)")
            
            print(f"âœ… WINDOW {W} å®Œæˆã€‚RMSE: {rmse:.3f}W")

    # è¼¸å‡ºæœ€çµ‚ç¸½çµå ±å‘Š
    print("\n\n======== å¯¦é©—ç¸½çµå ±å‘Š ========")
    results_df = pd.DataFrame(FINAL_RESULTS)

    if not results_df.empty:
        print(f"ç¸½çµæ¸¬è©¦çš„ WINDOWs: {WINDOW_LIST}")
        print(f"ðŸ† æœ€çµ‚æœ€ä½³ WINDOW: {BEST_WINDOW} åˆ†é˜ (RMSE: {BEST_RMSE:.3f}W)")
        
        # å¯«å…¥ CSV æª”æ¡ˆ
        results_df.to_csv(out_dir / "window_sensitivity_results.csv", index=False)
        print(f"ðŸ“ å¯¦é©—çµæžœå·²å„²å­˜è‡³: {out_dir / 'window_sensitivity_results.csv'}")
    else:
        print("âš ï¸ ç„¡æ³•é‹è¡Œä»»ä½• WINDOW å°ºå¯¸ã€‚æ•¸æ“šé‡éŽå°‘ã€‚")